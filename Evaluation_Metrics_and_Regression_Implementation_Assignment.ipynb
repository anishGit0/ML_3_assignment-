{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-m2R4dBkhTP"
      },
      "outputs": [],
      "source": [
        "# 2.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "X1 = 2 * np.random.rand(100, 1)\n",
        "X2 = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X1 + 2 * X2 + np.random.randn(100, 1)  # y = 4 + 3x1 + 2x2 + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X1, X2, y)), columns=['X1', 'X2', 'y'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data[['X1', 'X2']]\n",
        "y = data['y']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# 1. Check for linearity\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test['X1'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['X1'], y_pred, color='red', label='Predicted')\n",
        "plt.title('Linearity Check')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test['X2'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['X2'], y_pred, color='red', label='Predicted')\n",
        "plt.title('Linearity Check')\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Check for homoscedasticity\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Predicted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()\n",
        "\n",
        "# 3. Check for multicollinearity\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print error metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "id": "56JJ9kg5rCJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "data['target'] = y\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeRegressor(),\n",
        "    'Random Forest': RandomForestRegressor()\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "results = {}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Create a pipeline with feature scaling\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Fit the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate error metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    results[model_name] = {\n",
        "        'Mean Squared Error': mse,\n",
        "        'Mean Absolute Error': mae\n",
        "    }\n",
        "\n",
        "# Print the results\n",
        "for model_name, metrics in results.items():\n",
        "\n",
        "    print(f\"{model_name}:\")\n",
        "\n",
        "    print(f\"  Mean Squared Error (MSE): {metrics['Mean Squared Error']:.4f}\")\n",
        "\n",
        "    print(f\"  Mean Absolute Error (MAE): {metrics['Mean Absolute Error']:.4f}\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "jdiasOuoq9SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(X, columns=['Feature'])\n",
        "data['Target'] = y\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get model coefficients and intercept\n",
        "coefficients = model.coef_\n",
        "\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Coefficients: {coefficients[0]:.4f}\")\n",
        "\n",
        "print(f\"Intercept: {intercept:.4f}\")\n",
        "\n",
        "print(f\"R-squared score: {r_squared:.4f}\")"
      ],
      "metadata": {
        "id": "HfmhtxRmq4sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the tips dataset from seaborn\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(tips.head())\n",
        "\n",
        "# Prepare the data\n",
        "X = tips[['total_bill']]  # Independent variable\n",
        "y = tips['tip']           # Dependent variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the model coefficients and intercept\n",
        "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n",
        "print(f\"R-squared score: {r_squared:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot of total bill vs tip\n",
        "sns.scatterplot(data=tips, x='total_bill', y='tip', color='blue', label='Data Points')\n",
        "\n",
        "# Plot the regression line\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Total Bill vs Tip')\n",
        "plt.xlabel('Total Bill ($)')\n",
        "plt.ylabel('Tip ($)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QFQ0W50Dq0th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate values for the regression line\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "y_line = model.predict(X_line)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficient: {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
      ],
      "metadata": {
        "id": "QKX2khSKqwig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "y = 4 + 3 * X + 1.5 * X**2 + np.random.randn(100, 1)  # Quadratic relationship with noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_poly_train = poly_features.fit_transform(X_train)\n",
        "\n",
        "# Create and fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly_train, y_train)\n",
        "\n",
        "# Generate values for the regression curve\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "X_line_poly = poly_features.transform(X_line)  # Transform to polynomial features\n",
        "y_line = model.predict(X_line_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Polynomial Regression Curve (Degree 2)')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "c0x-yPYJqqnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 1) * 10  # 100 random values for X between 0 and 10\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2  # y = 2.5 * X + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model's coefficient and intercept\n",
        "coefficient = model.coef_[0][0]\n",
        "intercept = model.intercept_[0]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Coefficient: {coefficient:.4f}\")\n",
        "print(f\"Intercept: {intercept:.4f}\")"
      ],
      "metadata": {
        "id": "EtZu3DtCqmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "y = 4 + 3 * X + 1.5 * X**2 + np.random.randn(100, 1)  # Quadratic relationship with noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Degrees of polynomial to test\n",
        "degrees = [1, 2, 3]\n",
        "models = {}\n",
        "r2_scores = {}\n",
        "\n",
        "# Fit polynomial regression models of different degrees\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree)\n",
        "    X_poly_train = poly_features.fit_transform(X_train)\n",
        "    X_poly_test = poly_features.transform(X_test)\n",
        "\n",
        "    # Create and fit the linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores[degree] = r2\n",
        "    models[degree] = model\n",
        "\n",
        "    # Print the R-squared score\n",
        "    print(f\"Degree {degree} Polynomial Regression R-squared: {r2:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "\n",
        "for degree in degrees:\n",
        "    poly_features = PolynomialFeatures(degree=degree)\n",
        "\n",
        "    X_line_poly = poly_features.fit_transform(X_line)\n",
        "\n",
        "    y_line = models[degree].predict(X_line_poly)\n",
        "\n",
        "    plt.plot(X_line, y_line, label=f'Degree {degree} Polynomial')\n",
        "\n",
        "# Scatter plot of the original data\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points', alpha=0.5)\n",
        "\n",
        "plt.title('Polynomial Regression of Different Degrees')\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "\n",
        "plt.ylabel('Target')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPvVp1KxqhrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X1 = np.random.rand(100, 1) * 10  # Feature 1: 100 random values between 0 and 10\n",
        "X2 = np.random.rand(100, 1) * 5   # Feature 2: 100 random values between 0 and 5\n",
        "y = 3 + 2 * X1 + 1.5 * X2 + np.random.randn(100, 1)  # y = 3 + 2*X1 + 1.5*X2 + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X1, X2, y)), columns=['Feature1', 'Feature2', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data[['Feature1', 'Feature2']]\n",
        "y = data['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get the model's coefficients and intercept\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Coefficients: {coefficients}\")\n",
        "print(f\"Intercept: {intercept:.4f}\")\n",
        "print(f\"R-squared score: {r_squared:.4f}\")"
      ],
      "metadata": {
        "id": "z9XD40eYqc4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values for X between 0 and 2\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate values for the regression line\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "y_line = model.predict(X_line)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficient: {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
      ],
      "metadata": {
        "id": "r9ZVoG8oqXlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Create dummy variables for categorical features if necessary\n",
        "# Example: data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
        "\n",
        "# Define the independent variables\n",
        "X = data[['Feature1', 'Feature2', 'Feature3']]  # Replace with your feature names\n",
        "\n",
        "# Create a DataFrame to hold VIF values\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "\n",
        "# Display the VIF values\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "a9ku5Kb5qTO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values for X between 0 and 2\n",
        "y = 1 + 2 * X + 3 * X**2 - 4 * X**3 + 5 * X**4 + np.random.randn(100, 1) * 2  # Polynomial relationship with noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create polynomial features of degree 4\n",
        "poly_features = PolynomialFeatures(degree=4)\n",
        "X_poly_train = poly_features.fit_transform(X_train)\n",
        "\n",
        "# Create and fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly_train, y_train)\n",
        "\n",
        "# Generate values for the regression curve\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "X_line_poly = poly_features.transform(X_line)  # Transform to polynomial features\n",
        "y_line = model.predict(X_line_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Polynomial Regression Curve (Degree 4)')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "FK0egbajqO6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X1 = np.random.rand(100, 1) * 10  # Feature 1: 100 random values between 0 and 10\n",
        "X2 = np.random.rand(100, 1) * 5   # Feature 2: 100 random values between 0 and 5\n",
        "y = 3 + 2 * X1 + 1.5 * X2 + np.random.randn(100, 1) * 2  # y = 3 + 2*X1 + 1.5*X2 + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X1, X2, y)), columns=['Feature1', 'Feature2', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data[['Feature1', 'Feature2']]\n",
        "y = data['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a machine learning pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Step 1: Standardize the data\n",
        "    ('regressor', LinearRegression())  # Step 2: Fit the linear regression model\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"R-squared score: {r_squared:.4f}\")"
      ],
      "metadata": {
        "id": "STR1LfKoqJ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values for X between 0 and 2\n",
        "y = 1 - 2 * X + 3 * X**2 - 4 * X**3 + np.random.randn(100, 1) * 2  # Polynomial relationship with noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create polynomial features of degree 3\n",
        "poly_features = PolynomialFeatures(degree=3)\n",
        "X_poly_train = poly_features.fit_transform(X_train)\n",
        "\n",
        "# Create and fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly_train, y_train)\n",
        "\n",
        "# Generate values for the regression curve\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "X_line_poly = poly_features.transform(X_line)  # Transform to polynomial features\n",
        "y_line = model.predict(X_line_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Polynomial Regression Curve (Degree 3)')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "MICTsQwuqFMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 5) * 10  # 100 samples, 5 features (values between 0 and 10)\n",
        "coefficients = np.array([1.5, -2.0, 3.0, 0.5, -1.0])  # Coefficients for the features\n",
        "y = 5 + X @ coefficients + np.random.randn(n_samples) * 2  # y = 5 + linear combination + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(X, columns=[f'Feature{i+1}' for i in range(5)])\n",
        "data['Target'] = y\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score and model coefficients\n",
        "print(f\"R-squared score: {r_squared:.4f}\")\n",
        "print(f\"Model coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")"
      ],
      "metadata": {
        "id": "QEQnqQTlqAOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # 100 random values for X between 0 and 2\n",
        "y = 3 + 4 * X + np.random.randn(100, 1)  # y = 3 + 4x + noise\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(np.hstack((X, y)), columns=['Feature', 'Target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate values for the regression line\n",
        "X_line = np.linspace(0, 2, 100).reshape(-1, 1)  # 100 points from 0 to 2\n",
        "y_line = model.predict(X_line)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['Feature'], data['Target'], color='blue', label='Data Points')\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression Line')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(f\"Coefficient: {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
      ],
      "metadata": {
        "id": "M0pBlR8ypOll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "X1 = np.random.rand(n_samples) * 10  # Feature 1: 100 random values between 0 and 10\n",
        "X2 = np.random.rand(n_samples) * 5   # Feature 2: 100 random values between 0 and 5\n",
        "X3 = np.random.rand(n_samples) * 20  # Feature 3: 100 random values between 0 and 20\n",
        "\n",
        "# Coefficients for the features\n",
        "coefficients = np.array([1.5, -2.0, 3.0])\n",
        "# Generate target variable with some noise\n",
        "y = 5 + 1.5 * X1 - 2.0 * X2 + 3.0 * X3 + np.random.randn(n_samples) * 2\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': X1,\n",
        "    'Feature2': X2,\n",
        "    'Feature3': X3,\n",
        "    'Target': y\n",
        "})\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data[['Feature1', 'Feature2', 'Feature3']]\n",
        "y = data['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score and model coefficients\n",
        "print(f\"R-squared score: {r_squared:.4f}\")\n",
        "print(f\"Model coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")"
      ],
      "metadata": {
        "id": "DObfstN1pcTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "import joblib\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Serialize (save) the model using joblib\n",
        "model_filename = 'linear_regression_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "# Deserialize (load) the model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\", y_pred)\n",
        "\n",
        "# Optionally, you can also evaluate the model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "ArgXjJmbskgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the tips dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(tips.head())\n",
        "\n",
        "# One-hot encode the categorical features\n",
        "tips_encoded = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n",
        "\n",
        "# Define the independent variables (X) and the dependent variable (y)\n",
        "X = tips_encoded.drop('total_bill', axis=1)  # Independent variables\n",
        "y = tips_encoded['total_bill']  # Dependent variable\n",
        "\n",
        "# Add a constant to the independent variables for the intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the summary of the regression results\n",
        "print(model.summary())\n",
        "\n",
        "# Optional: Visualize the relationship between total_bill and other features\n",
        "sns.scatterplot(data=tips, x='total_bill', y='tip', hue='sex')\n",
        "plt.title('Total Bill vs Tip by Sex')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3JsbH1ujpnh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23.\n",
        "\n",
        "# Fit Ridge Regression model\n",
        "ridge_reg = Ridge(alpha=1.0)  # You can adjust alpha for regularization strength\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ridge = ridge_reg.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# Print coefficients and R-squared score\n",
        "print(\"Ridge Regression Coefficients:\", ridge_reg.coef_)\n",
        "print(\"Ridge Regression Intercept:\", ridge_reg.intercept_)\n",
        "print(\"Ridge Regression R-squared:\", r2_ridge)"
      ],
      "metadata": {
        "id": "PhFiFi4BrWBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10  # 100 samples, single feature\n",
        "y = 2.5 * X.squeeze() + np.random.randn(100) * 2  # Linear relationship with noise\n",
        "\n",
        "# Create a DataFrame for better visualization (optional)\n",
        "data = pd.DataFrame(data={'Feature': X.squeeze(), 'Target': y})\n",
        "print(data.head())\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Convert negative MSE to positive MSE\n",
        "cv_scores = -cv_scores\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"Cross-Validation Mean Squared Errors:\", cv_scores)\n",
        "print(\"Mean CV MSE:\", np.mean(cv_scores))\n",
        "\n",
        "# Fit the model on the entire training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error on the test set\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test Mean Squared Error:\", test_mse)\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "id": "4aVFPGyGsKAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10  # 100 samples, single feature\n",
        "y = 2.5 * (X.squeeze() ** 2) + np.random.randn(100) * 5  # Quadratic relationship with noise\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List to store R-squared scores\n",
        "r2_scores = []\n",
        "\n",
        "# Degrees of polynomial to test\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Fit polynomial regression models of different degrees\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "\n",
        "    # Fit the model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "    # Print the R-squared score for the current degree\n",
        "    print(f\"Degree: {degree}, R-squared: {r2:.4f}\")\n",
        "\n",
        "# Optional: Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(degrees, r2_scores, marker='o')\n",
        "plt.title('R-squared Scores for Polynomial Regression Models')\n",
        "plt.xlabel('Polynomial Degree')\n",
        "plt.ylabel('R-squared Score')\n",
        "plt.xticks(degrees)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s_PnEiTbsT3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoritical**"
      ],
      "metadata": {
        "id": "NJkasbdutNuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance for a dependent variable that is explained by one or more independent variables in a regression model."
      ],
      "metadata": {
        "id": "iY92j6mTtQqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Linear regression is a widely used statistical method for modeling the relationship between a dependent variable and one or more independent variables. However, for the results of a linear regression analysis to be valid, certain assumptions must be met. Here are the key assumptions of linear regression:\n",
        "\n",
        "  1. Linearity.\n",
        "  2. Independence.\n",
        "  3. Homoscedasticity.\n",
        "  4. Normality of Residuals.\n",
        "  5. No Multicollinearity.\n",
        "  6. No Autocorrelation."
      ],
      "metadata": {
        "id": "M5GZdKRxtuje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. R-squared and Adjusted R-squared are both statistical measures used to evaluate the goodness of fit of a regression model, but they serve slightly different purposes and have different interpretations. Heres a breakdown of the differences between the two:\n",
        "\n",
        "  1. Use R-squared when you want a simple measure of how well your model explains the variability in the dependent variable.\n",
        "  \n",
        "  2. Use Adjusted R-squared when you want to compare models with different numbers of predictors or when you want to avoid the pitfalls of overfitting."
      ],
      "metadata": {
        "id": "1qG_h2Wzu-ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Mean Squared Error (MSE) is a widely used metric for evaluating the performance of regression models. Here are several reasons why MSE is commonly used:\n",
        "\n",
        "  1. Quantifies Prediction Error.\n",
        "  2. Emphasizes Larger Errors.\n",
        "  3. Differentiability.\n",
        "  4. Interpretability.\n",
        "  5. Widely Accepted.\n",
        "  6. Useful for Model Comparison.\n",
        "  "
      ],
      "metadata": {
        "id": "YBsivcX-vMpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The model explains 85% of the variability in the data. The remaining 15% is likely due to factors not included in the model or random chance."
      ],
      "metadata": {
        "id": "htyzzoOJwD_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. To check for normality of residuals in linear regression:\n",
        "\n",
        "  1. Visual Inspection:\n",
        "\n",
        "    Histogram: Plot a histogram of the residuals. If it resembles a bell curve, it suggests normality.\n",
        "  \n",
        "    Q-Q Plot: Create a Q-Q plot of the residuals. If the points fall approximately along a straight diagonal line, it indicates normality.\n",
        "\n",
        "  2. Statistical Tests:\n",
        "\n",
        "    Shapiro-Wilk Test: Perform the Shapiro-Wilk test using shapiro.test() in R or Python's scipy.stats. A p-value greater than your significance level (e.g., 0.05) suggests normality.\n"
      ],
      "metadata": {
        "id": "XLSiYoKow0mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other. This means they provide similar information about the dependent variable.\n",
        "\n",
        "  Impact on Regression:\n",
        "\n",
        "  1. Unstable Coefficients: Multicollinearity makes the estimated regression coefficients unstable and difficult to interpret. Small changes in the data can lead to large changes in the coefficients.\n",
        "\n",
        "  2. Reduced Significance: It can inflate the standard errors of the coefficients, making it harder to detect statistically significant relationships between predictors and the outcome.\n",
        "\n",
        "  3. Difficult Interpretation: Multicollinearity makes it challenging to isolate the individual effects of each predictor on the dependent variable."
      ],
      "metadata": {
        "id": "SwWMnjxXxQAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Mean Absolute Error (MAE)** is a metric used to measure the average absolute difference between predicted and actual values in a regression model. It provides a measure of the overall prediction accuracy, focusing on the magnitude of errors rather than their direction."
      ],
      "metadata": {
        "id": "Sckzvn11xmhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Here are the benefits of using an ML pipeline in short:\n",
        "\n",
        "  Reproducibility: Ensures consistent and repeatable results.\n",
        "\n",
        "  Maintainability: Makes the workflow easier to update and manage.\n",
        "\n",
        "  Modularity: Allows for easy swapping of components and experimentation.\n",
        "\n",
        "  Efficiency: Automates the process, saving time and resources.\n",
        "\n",
        "  Scalability: Enables handling large datasets and complex models."
      ],
      "metadata": {
        "id": "Uisj433Mxyaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Here's why RMSE is considered more interpretable than MSE in short:\n",
        "\n",
        "  Same Units: RMSE is in the same units as the target variable, making it easier to understand in the context of the problem.\n",
        "\n",
        "  Relatable to Data: RMSE represents the average magnitude of errors, giving a more intuitive sense of the model's prediction accuracy."
      ],
      "metadata": {
        "id": "QyoKHppZyCEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Pickling is the process of converting a Python object into a byte stream (serialization) to store it on disk or transfer it over a network. This byte stream can later be converted back into the original Python object (deserialization).\n",
        "\n",
        "  Usefulness in ML:\n",
        "\n",
        "      Pickling is extremely useful in machine learning for saving trained models, allowing you to reuse them later without retraining. This saves time and computational resources. You can also use pickling to store intermediate data or complex objects for later use."
      ],
      "metadata": {
        "id": "OrUBfXpsyVxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. A high R-squared value indicates that the independent variables in a regression model explain a large proportion of the variance in the dependent variable."
      ],
      "metadata": {
        "id": "cmKyeva4yyCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Violating linear regression assumptions can lead to:\n",
        "\n",
        "  1. Inaccurate Estimates: The estimated coefficients and predictions may be biased or inefficient.\n",
        "\n",
        "  2. Invalid Inferences: Hypothesis tests and confidence intervals may be unreliable, leading to incorrect conclusions about the relationships between variables.\n",
        "\n",
        "  3. Misleading Predictions: Predictions made by the model may be inaccurate or have larger errors.\n",
        "\n",
        "  4. Reduced Model Performance: The overall performance of the model may be compromised, affecting its ability to generalize to new data."
      ],
      "metadata": {
        "id": "bMlwLZvYy_gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Here's how to address multicollinearity in regression, in short:\n",
        "\n",
        "  1. Remove Correlated Variables: Identify and remove one or more of the highly correlated independent variables.\n",
        "\n",
        "  2. Combine Variables: Create a composite variable by combining the correlated variables (e.g., using principal component analysis).\n",
        "\n",
        "  3. Regularization: Use regularization techniques like Ridge regression or Lasso regression to shrink the coefficients and reduce the impact of multicollinearity.\n",
        "\n",
        "  4. Collect More Data: Increasing the sample size can sometimes help reduce the effects of multicollinearity."
      ],
      "metadata": {
        "id": "yZkbhm9SzQi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Here's how feature selection can improve model performance in regression analysis, in short:\n",
        "\n",
        "  1. Reduces Overfitting: By removing irrelevant features, the model focuses on the most important predictors, reducing the risk of overfitting to noise in the data.\n",
        "\n",
        "  2. Improves Accuracy: Including only relevant features can lead to more accurate predictions, as the model is not distracted by irrelevant information.\n",
        "  \n",
        "  3. Simplifies Model: A simpler model with fewer features is easier to interpret and understand, making it more useful for insights and decision-making.\n",
        "\n",
        "  4. Reduces Computational Cost: Training and prediction times are reduced with fewer features, making the model more efficient."
      ],
      "metadata": {
        "id": "lNttFYjEzlOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Adjusted R-squared is calculated using the following formula:\n",
        "\n",
        "    Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
        "\n",
        "  Where:\n",
        "\n",
        "  R-squared: The regular R-squared value.\n",
        "\n",
        "  n: The number of observations (data points).\n",
        "\n",
        "  k: The number of predictors (independent variables)."
      ],
      "metadata": {
        "id": "5v3NHQzAz3fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. MSE squares the errors, giving larger errors disproportionately more weight. Outliers, being extreme values, have large errors, and when squared, these errors become even larger, significantly inflating the MSE.\n",
        "\n",
        "  Outliers have a big impact on MSE because it amplifies the effect of large errors through squaring."
      ],
      "metadata": {
        "id": "-PqZuK2l02hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Homoscedasticity means that the variance of the errors (residuals) is constant across all levels of the independent variables. This is a key assumption in linear regression.\n",
        "\n",
        "  Importance:\n",
        "\n",
        "  1. Valid inferences: Homoscedasticity ensures that the standard errors of the regression coefficients are unbiased, allowing for valid hypothesis testing and confidence intervals.\n",
        "\n",
        "  2. Reliable predictions: It helps in producing reliable predictions across the range of independent variables.\n",
        "  \n",
        "  3. Efficient estimates: Homoscedasticity leads to efficient estimates of the regression coefficients, meaning they are more precise."
      ],
      "metadata": {
        "id": "KPeZNOrd1FP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Root Mean Squared Error (RMSE) is a metric that measures the average magnitude of the errors (residuals) in a regression model. It's essentially the square root of the Mean Squared Error (MSE).\n",
        "\n",
        "  RMSE = ((yi - i) / n)\n",
        "\n",
        "  Where:\n",
        "\n",
        "  yi: Actual value for the ith data point\n",
        "\n",
        "  i: Predicted value for the ith data point\n",
        "\n",
        "  n: Total number of data points"
      ],
      "metadata": {
        "id": "C5CZAPhe1Z19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Pickling can be risky due to potential security vulnerabilities. Unpickling untrusted data can lead to arbitrary code execution, allowing malicious actors to compromise your system."
      ],
      "metadata": {
        "id": "oQbKDx5i1rzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Here are some alternatives to pickling for saving ML models in short:\n",
        "\n",
        "  1. Joblib: Offers efficient serialization for NumPy arrays, making it suitable for scikit-learn models.\n",
        "\n",
        "  2. ONNX: Creates a portable, open-source format for representing machine learning models, enabling interoperability between different frameworks.\n",
        "  \n",
        "  3. PMML: Provides a standardized XML-based language for describing and exchanging predictive models.\n",
        "  \n",
        "  4. Saving model weights directly: Deep learning frameworks often allow saving model weights as separate files (e.g., .h5 for Keras models).\n"
      ],
      "metadata": {
        "id": "SyhyyhEh11DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Heteroscedasticity is when the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables. This violates a key assumption of linear regression, which assumes homoscedasticity (constant variance).\n",
        "\n",
        "  Why is it a problem?\n",
        "\n",
        "  Heteroscedasticity can lead to:\n",
        "\n",
        "  1. Inefficient Estimates: The estimated regression coefficients become less precise and reliable.\n",
        "  \n",
        "  2. Invalid Inferences: Hypothesis tests and confidence intervals based on these estimates may be misleading.\n",
        "  \n",
        "  3. Biased Standard Errors: Standard errors are underestimated, potentially leading to incorrect conclusions about the significance of predictors."
      ],
      "metadata": {
        "id": "cCivRrdk2LYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Interaction terms allow the model to capture the combined effect of two or more predictors on the outcome variable. This can significantly improve prediction accuracy when the relationship between predictors and the outcome is not simply additive."
      ],
      "metadata": {
        "id": "4edhDuxd2gqS"
      }
    }
  ]
}